---
layout: post
title: CUDA-Operators-4-softmax
date: 2025-09-13 08:48 +0000
categories: [CUDA]
tags: [CUDA]
pin: false
math: true
mermaid: true
---

## 1. Softmax åŸºæœ¬å®ç°
Softmax å°†ä¸€ä¸ªæ•°å€¼å‘é‡å½’ä¸€åŒ–ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå‘é‡ï¼Œä¸”å„ä¸ªæ¦‚ç‡ä¹‹å’Œä¸º 1ã€‚Softmax å¯ä»¥ç”¨æ¥ä½œä¸ºç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚ï¼Œç”¨äºå¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºã€‚

åŸå§‹ softmax ä¸­ $\sum e^{x_i}$ å®¹æ˜“å¯¼è‡´æ•°å€¼æº¢å‡ºï¼Œé€šå¸¸ä½¿ç”¨ safe softmaxï¼Œå³è®© $x_i - max{x}$ï¼Œä»¥é˜²æ­¢æ•°å€¼æº¢å‡ºï¼Œå…·ä½“å…¬å¼è¡¨è¾¾å¦‚ä¸‹æ‰€ç¤ºã€‚

$$
m = max(x)
$$

$$
Softmax(x) = \frac{e^{x_i-m}}{\sum e^{x_i-m}} 
$$

Softmax ä¸­éœ€è¦æ‰§è¡Œä¸‰æ¬¡å¾ªç¯ï¼Œæœ€å…³é”®æ˜¯ max å’Œ sum ä¸¤ä¸ª reduce æ“ä½œã€‚è¦å®Œæˆè®¡ç®—éœ€è¦è¯»å…¥ä¸‰æ¬¡ï¼Œå†™ä¸€æ¬¡ï¼Œç®—æ³•å¦‚ä¸‹æ‰€ç¤ºã€‚

<div style="border:1px solid #aaa; padding:10px; border-radius:6px; background:#f9f9f9; font-family:monospace;">

<b>Algorithm 1: Safe Softmax</b>  <br/>

1&nbsp; $m \gets -\infty$  <br/>
2&nbsp; <b>for</b> $i \gets 1$ to $N$ <b>do</b> <br/>
3&nbsp;   &nbsp;&nbsp;&nbsp;$m \gets \max(m, x_i)$  <br/>
4&nbsp; $s \gets 0$  <br/>
5&nbsp; <b>for</b> $i \gets 1$ to $N$ <b>do</b> <br/>  
6&nbsp;   &nbsp;&nbsp;&nbsp;$s \gets s + e^{x_i - m}$  <br/>
7&nbsp; <b>for</b> $i \gets 1$ to $N$ <b>do</b> <br/>  
8&nbsp;   &nbsp;&nbsp;&nbsp;$y_i \gets \dfrac{e^{x_i - m}}{s}$  

</div>

### 1.1 Naive GPU å®ç° (V1)
ç”±äº Norm è¿‡ç¨‹ä¾èµ–å½’çº¦çš„ç»“æœï¼Œæœ€ Naive çš„å®ç°å°±æ˜¯ä¸€ä¸ªçº¿ç¨‹ç®—ç»“æœä¸­ä¸€è¡Œï¼Œä¸€ä¸ª block ç®— `BLOCK_SIZE` è¡Œï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚

```c
/**
 * Every block calculate `BLOCK_SIZE` lines of result, one thread calculate one line
 * inp is (N, C)
 * out is (N, C)
 * BLOCK_SIZE = 32
 * dim3 block(BLOCK_SIZE)
 * dim3 grid((N + BLOCK_SIZE -1) / BLOCK_SIZE)
 */
__global__ void softmax_forward_naive_f32_kernel(float* out, const float* inp, int N, int C) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        const float* inp_row = inp + i * C;
        float* out_row = out + i * C;

        float maxval = -INFINITY;
        for (int j = 0; j < C; j++) {
            if (inp_row[j] > maxval) {
                maxval = inp_row[j];
            }
        }
        double sum = 0.0;
        for (int j = 0; j < C; j++) {
            out_row[j] = expf(inp_row[j] - maxval);
            sum += out_row[j];
        }
        for (int j = 0; j < C; j++) {
            out_row[j] /= (float)sum;
        }
    }
}
```

### 1.2 Shared mem å®ç° (V2)
V1 ç‰ˆæœ¬å®ç°è¿‡äº Naiveï¼Œæ¯ä¸ªçº¿ç¨‹ç®—ä¸€è¡Œï¼Œå¯¼è‡´ï¼šï¼ˆ1ï¼‰å•ä¸ªçº¿ç¨‹ä»»åŠ¡å¾ˆé‡ï¼Œå¹¶è¡Œæ€§ä¸è¶³ï¼›ï¼ˆ2ï¼‰ä¸‰æ¬¡è®¿å­˜éƒ½ä» global mem å–ï¼Œè®¿å­˜æ•ˆç‡ä½ã€‚

ä¸‹é¢çš„ä¼˜åŒ–ç‰ˆæœ¬ä¸­ï¼Œè®©æ¯ä¸ª block è®¡ç®—ä¸€è¡Œå…ƒç´ ï¼Œå¹¶ä½¿ç”¨ shared mem ç¼“å­˜**å½’çº¦ç»“æœ**ã€‚ä»£ç å¦‚ä¸‹æ‰€ç¤º

```c
/**
 * Every block calculate one line.
 * inp is (N, C)
 * 
 * BLOCK_SZIE = 512
 * dim3 grid(N)
 * dim3 block(BLOCK_SIZE)
 * size_t smem_size = BLOCK_SIZE * sizeof(float)
 * softmax_forward_smem_f32_kernel<<<grid, block, smem_size>>>(out, inp, N, C);
 */
__global__ void softmax_forward_smem_f32_kernel(float* out, const float* inp, int N, int C) {
  extern __shared__ float shared[];
  int idx = blockIdx.x;   // ranges [0, N)
  int tid = threadIdx.x;  // ranges [0, block_size)
  int block_size = blockDim.x;
  const float* x = inp + idx * C;

  float maxval = -INFINITY;
  for (int i = tid; i < C; i += block_size) {
      maxval = fmaxf(maxval, x[i]);
  }
  shared[tid] = maxval;
  __syncthreads();

  // max reductions
  for (int stride = block_size / 2; stride >= 1; stride /= 2) {
      if (tid < stride) {
          shared[tid] = fmaxf(shared[tid], shared[tid + stride]);
      }
      __syncthreads();
  }

  float offset = shared[0];
  float sumval = 0.0f;
  for (int i = tid; i < C; i += block_size) {
      sumval += expf(x[i] - offset);
  }
  shared[tid] = sumval;
  __syncthreads();
  
  // sum reduction
  for (int stride = block_size / 2; stride >= 1; stride /= 2) {
      if (tid < stride) {
          shared[tid] += shared[tid + stride];
      }
      __syncthreads();
  }

  // broadcast the sum to all threads in the block
  float sum = shared[0];
  for (int i = tid; i < C; i += block_size) {
      out[idx * C + i] = expf(x[i] - offset) / sum;
  }
}
```

### 1.3 Shared mem + Warp Reduce ä¼˜åŒ– (V3)

å½’çº¦è¿‡ç¨‹å¯ä»¥ä½¿ç”¨ warp reduce ç›´æ¥åœ¨å¯„å­˜å™¨ä¸­å®ç°ä¼˜åŒ–ï¼Œåˆ†å—ä¸º `grid((N + BN - 1) / BN)` å’Œ `block(BC, BN)`ã€‚å³æ¯ä¸ª block è®¡ç®— BN è¡Œç»“æœï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç† (C + BC - 1) / BC ä¸ªæ•°ï¼Œçº¿ç¨‹è§†è§’çš„å½’çº¦è®¡ç®—æµç¨‹å¦‚ä¸‹ã€‚

* æ¯ä¸ªçº¿ç¨‹è®¡ç®— (C + BC - 1) / BC ä¸ªæ•°ï¼Œå³åˆæ­¥å½’çº¦åˆ° 0...BC-1 çº¿ç¨‹ï¼›
* å¯¹ 0...BC-1 çº¿ç¨‹çš„æ¯ä¸ª warp åš warp reduceï¼Œæ¯ä¸ª warp çš„ç»“æœç¼“å­˜åœ¨ smem ä¸­ï¼›
* ç„¶å 0 å·çº¿ç¨‹å½’çº¦æ¯ä¸ª warp çš„ç»“æœï¼Œå¾—åˆ°å½’çº¦ç»“æœï¼Œå†™å…¥ 0 å·çº¿ç¨‹å¯¹åº”çš„ smem ä¸­ï¼›
* éœ€è¦ä½¿ç”¨æ—¶ï¼Œä» smem ä¸­å–å‡ºï¼Œå³ smem å¹¿æ’­çš„è¿‡ç¨‹ï¼›

![Desktop View](/assets/img/blog/CUDA/shfl_down.png){: width="500" height="500" }
<center>Fig 1. Warp Reduction ç¤ºä¾‹</center>

å…·ä½“ä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚
```c
#define WarpSize 32
/**
 * Every Block calculate BN lines
 * BN = 4
 * BC = 256 -> a multiple of 32
 * inp(N, C)
 * grid((N + BN - 1) / BN)
 * block(BC, BN)
 */
template <const int BN, const int BC>
__global__ void softmax_forward_warp_smem_f32_kernel(float* out, const float* inp, int N, int C){
  // shared memory is used for inter-warp reduction
  __shared__ float smem[BN][2][BC];
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int bx = blockIdx.x;

  int n = bx * BN + ty;
  int lane_id = tx % WarpSize;
  int warp_id = tx / WarpSize;

  if (n >= N) return;

  const int niter = BC / WarpSize;
  const float* gmem = inp + n * C;

  // 1.max reduction
  float max = -INFINITY;
  for (int i = tx; i < C; i += BC){
    max = fmaxf(max, gmem[i]);
  }
  
  max = fmaxf(max, __shfl_down_sync(0xffffffff, max, 16));
  max = fmaxf(max, __shfl_down_sync(0xffffffff, max, 8));
  max = fmaxf(max, __shfl_down_sync(0xffffffff, max, 4));
  max = fmaxf(max, __shfl_down_sync(0xffffffff, max, 2));
  max = fmaxf(max, __shfl_down_sync(0xffffffff, max, 1));

  if (lane_id == 0) smem[ty][0][warp_id] = max;
  __syncthreads();

  if (tx == 0) {
    int maxval = smem[ty][0][0];
    for (int i = 1; i < niter; ++i){
      maxval = fmaxf(maxval, smem[ty][0][i]);
    }
    smem[ty][0][0] = maxval;
  }
  __syncthreads();

  // 2.sum reduction
  float offset = smem[ty][0][0];  // broadcast maxval
  float sum = 0;
  for (int i = tx; i < C; i += BC){
    sum += expf(gmem[i] - offset);
  }

  sum += __shfl_xor_sync(0xffffffff, sum, 16);
  sum += __shfl_xor_sync(0xffffffff, sum, 8);
  sum += __shfl_xor_sync(0xffffffff, sum, 4);
  sum += __shfl_xor_sync(0xffffffff, sum, 2);
  sum += __shfl_xor_sync(0xffffffff, sum, 1);

  if (lane_id == 0) smem[ty][1][warp_id] = sum;
  __syncthreads();
  if (tx == 0){
    float sumval = smem[ty][1][0];
    for (int i = 1; i < niter; ++i){
      sumval += smem[ty][1][i];
    }
    smem[ty][1][0] = sumval;
  }
  __syncthreads();

  // 3.norm
  float sumval = smem[ty][1][0];
  for (int i = tx; i < C; i += BC){
    out[n * C + i] = expf(gmem[i] - offset) / sumval;
  }
}
```

## 2. Softmax é«˜é˜¶ä¼˜åŒ–

Online softmax [4] é€šè¿‡ç´¯åŠ è¿­ä»£è®¡ç®—ï¼Œèƒ½å¤Ÿ**çœç•¥ä¸€æ¬¡å¾ªç¯**å¹¶å®ç° safe softmax çš„ç­‰ä»·è®¡ç®—ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨æŒ‡æ•°è®¡ç®—ç‰¹æ€§ï¼ŒæŠŠç¬¬ i æ­¥ max å€¼å¥—å…¥åˆ° i-1 æ­¥ sum exp ä¸­ï¼Œå†åŠ ä¸Šå½“å‰çš„ $exp(x_i-max_i)$ å³ä¸ºç¬¬ i æ­¥ sum expã€‚

<div style="border:1px solid #aaa; padding:10px; border-radius:6px; background:#f9f9f9; font-family:monospace;">
<b>Algorithm 2: Online Softmax</b> <br/>
1&nbsp; $m_0 \gets -\infty$ <br/>
2&nbsp; $s_0 \gets 0$ <br/>
3&nbsp; <b>for</b> $i \gets 1$ to $N$ <b>do</b> <br/>
4&nbsp;&nbsp;&nbsp;$m_i \gets \max(m_{i-1}, x_i)$ <br/>
5&nbsp;&nbsp;&nbsp;$s_i \gets s_{i-1} \cdot e^{\,m_{i-1} - m_i\,} + e^{\,x_i - m_i\,}$ <br/>
6&nbsp; <b>end for</b> <br/>
7&nbsp; <b>for</b> $i \gets 1$ to $N$ <b>do</b> <br/>
8&nbsp;&nbsp;&nbsp;$y_i \gets \dfrac{e^{x_i - m_N}}{s_N}$ <br/>
9&nbsp; <b>end for</b>
</div>

> NOTE: logSoftmax åŒæ ·èƒ½ç”¨ä¸Šé¢çš„ sum exp æ€§è´¨è¿›è¡Œä¼˜åŒ–ã€‚

$$
logSoftmax(x) = log(\frac{e^{x_i-x_{max}}}{\sum_j(e^{x_j-x_{max}})}) = x_i - x_m -log(\sum_j e^{x_j-x_{max}})
$$

### 2.1 åä½œç»„ + ç»“æ„ä½“å®ç° (V4)

ä¸‹é¢æ˜¯ åä½œç»„ + ç»“æ„ä½“ çš„ç®€å•å®ç°ï¼Œå½“ C ç»´åº¦è¾ƒå¤§æ—¶ï¼Œå•ä¸ªçº¿ç¨‹ä»»åŠ¡è¿‡é‡ï¼Œå¯¼è‡´çº¿ç¨‹èµ„æºç´§å¼ ï¼Œå¹¶è¡Œæ€§ä¼šä¸‹é™ã€‚

```c
#include <cooperative_groups.h>
#include <cooperative_groups/reduce.h>
namespace cg = cooperative_groups;

struct __align__(8) SumMax
{
  float maxval;
  float sum;
};

__device__ __forceinline__ SumMax reduce_sum_max_op(SumMax a, SumMax b) {
  bool a_bigger = (a.maxval > b.maxval);
  SumMax bigger_m = a_bigger ? a : b;
  SumMax smaller_m = a_bigger ? b : a;
  SumMax res;
  res.maxval = bigger_m.maxval;
  res.sum = bigger_m.sum + smaller_m.sum * expf(smaller_m.maxval - bigger_m.maxval);
  return res;
}

/**
 * Every warp calculate one line.
 * BLOCK â†’ a multiple of 32
 * dim3 grid((N + BLOCK_SIZE / 32 - 1) / (BLOCK_SIZE / 32))
 * dim3 block(BLOCK_SIZE)
 */
__global__ void softmax_forward_online_kernel2(float* out, const float* inp, int N, int C) {
	cg::thread_block block = cg::this_thread_block();
	cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);
	int idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();
	if (idx >= N) {
		return;
	}
	const float* x = inp + idx * C;
  SumMax sm_partial;
	sm_partial.maxval = -INFINITY;
	sm_partial.sum = 0.0f;

	for (int i = warp.thread_rank(); i < C; i += warp.size()) {
		sm_partial = reduce_sum_max_op(sm_partial, { x[i], 1.0f });
	}

	SumMax sm_total = cg::reduce(warp, sm_partial, reduce_sum_max_op);

  // __stcs â†’ st.global.cs SASS å¿½ç•¥ L1ï¼Œå†™åˆ° global mem ï¼ˆåªèµ° L2/DRAMï¼‰
	for (int i = warp.thread_rank(); i < C; i += warp.size()) {
    __stcs(out + idx * C + i, expf(x[i] - sm_total.maxval) / sm_total.sum);
	}
}
```

ä¸Šé¢çš„ä»£ç éš¾ç‚¹åœ¨ `reduce_sum_max_op` å‡½æ•°ä¸­ï¼Œè¿™ä¸ª reduce ä»£ç æœ¬è´¨æ˜¯åˆå¹¶ä¸¤æ®µ exp sumã€‚å‡è®¾ A å’Œ B æ˜¯ä¸¤ä¸ª SumMax æ•°æ®ç»“æ„ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ±‚ A å’Œ B æœ€å¤§å€¼ä¸‹çš„ safe exp sumã€‚

$$
A.sum = \sum e^{x_i-a.maxval},x_i\in A
$$

$$
B.sum = \sum e^{x_j-b.maxval},x_j\in B
$$

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ¯”è¾ƒæœ€å¤§å€¼ï¼Œ**å‡è®¾ A å¤§**ï¼Œæ­¤æ—¶æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°† B è½¬åŒ–ä¸º $\sum e^{x_j-a.maxval}$ å½¢å¼ã€‚å› æ­¤ï¼Œæ ¹æ®æŒ‡æ•°æ€§è´¨ï¼Œå¾—ï¼š

$$
 \sum e^{x_j-a.maxval} = \sum e^{x_j-b.maxval}\cdot e^{b.maxval-a.maxval} = b.sum\cdot e^{b.maxval-a.maxval}
$$
 
 æ‰€ä»¥åˆå¹¶è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š

$$
sum'= a.sum + b.sum\cdot e^{b.maxval-a.maxval}
$$

## Reference
[1] [LeetCUDA](https://github.com/xlite-dev/LeetCUDA.git.)

[2] [ops(2)ï¼šSoftMaxç®—å­çš„ CUDA å®ç°.](https://zhuanlan.zhihu.com/p/695307283)

[3] [Attentionä¼˜åŒ–\]\[2wå­—\]ğŸ“šåŸç†ç¯‡: ä»Online-Softmaxåˆ°FlashAttention V1/V2/V3.](https://zhuanlan.zhihu.com/p/668888063)

[4] [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867)
