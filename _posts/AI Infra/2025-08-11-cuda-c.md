---
layout: post
title: CUDA-C
date: 2025-08-11 11:38 +0000
categories: [CUDA]
tags: [CUDA, basic]
pin: true
math: true
mermaid: true
---

## 1. CUDA 编程结构
GPU 编程通常是异构环境（多个CPU，GPU），CPU 和 GPU 通过 PCIe 总线相互通信，也是通过 PCIe 总线分隔开的。所以，我们需要区分**CPU及其内存** 和 **GPU及其内存**。

​注意，目前不考虑统一寻址，调试程序在编写时，内存调度采用在 CPU（Host）和 GPU（Device）来回 copy 的方法。

​一种完整的 CUDA 应用可能的执行顺序如 <i>Fig 1</i> 所示，<b>核函数被调用后，控制权马上归还主机线程</b>，即：第一段 Device 并行代码执行时，第二段 Host 代码可能已经同步执行了）。
![Desktop View](/assets/img/blog/CUDA/host-device.png){: width="500" height="350" }
<center><i>Fig 1.</i> CUDA应用可能的执顺序</center>

CUDA 编程结构主要涉及四个方面：内存、线程、核函数、错误处理。下面将分别介绍。

### 1.1 内存
CUDA提供的API可以分配管理**设备(Device)**上的内存，API接口如下所示。

| 标准C函数 | CUDA C 函数 |   说明   |
| :-------: | :---------: | :------: |
|  malloc   | cudaMalloc  | 内存分配 |
|  memcpy   | cudaMemcpy  | 内存复制 |
|  memset   | cudaMemset  | 内存设置 |
|   free    |  cudaFree   | 释放内存 |

`cudaMemcpy` 最关键，是内存拷贝的过程（走总线），可以完成以下几种拷贝过程（cudaMemcpyKind kind）。内存拷贝的方向如字面所示，如果函数执行成功，则会返回 `cudaSuccess`，否则返回 `cudaErrorMemoryAllocation`。

```c++
cudaError_t cudaMemcpy(void * dst,const void * src,size_t count, cudaMemcpyKind kind)
/** cudaMemcpyKind 类型：
* cudaMemcpyHostToHost
* cudaMemcpyHostToDevice
* cudaMemcpyDeviceToHost
* cudaMemcpyDeviceToDevice 
*/
```

GPU 内存层次较为复杂，内存层次关系如下图 *Fig 2.* 所示，其中，共享内存（shared Memory）和全局内存（global Memory）是能够显示控制的，后续会进行详细介绍。

![Desktop View](/assets/img/blog/CUDA/mem.png){: width="780" height="450" }
<center><i>Fig 2.</i> CUDA 内存层次 overview</center>

### 1.2 线程
CUDA 是 SIMT（Single Instruction, Multi Threads）架构，多个线程执行同一份代码，但每个线程拥有独立的寄存器、程序计数器和状态，可根据自身数据条件（如分支判断）选择执行或跳过指令。

“同一份代码” 则是核函数。从线程层次角度而言，一个核函数只能有 1 个 grid，1 个 grid 可以有多个 block，每个 block 可以有很多的线程。即 grid 在编程中的表达形式：`<block_num_dim3, thread_num_dim3>`，具体层次结构如 *Fig 3* 所示。

![Desktop View](/assets/img/blog/CUDA/thread.png){: width="400" height="450" }
<center><i>Fig 3.</i> CUDA 线程层次结构 overview</center>

<b>每个线程都执行同一份的串行代码</b>，怎么让这段相同的代码对应不同的数据呢？使用标记，让块内线程彼此区分开。CUDA 中依靠 `blockIdx` (线程块 block 在线程网格 grid 内的位置索引)和 `threadIdx` (线程 thread 在线程块 block 内的位置索引)。两个结构体基于 uint3 的定义，包含三个无符号整数结构。

``` c++
struct __device_builtin__ dim3
{
    unsigned int x, y, z;
}
```

需要注意，**不同 block 的线程是物理隔离的，不能相互影响。** 相同 block 内的线程可以完成：（1）同步；（2）共享内存。

### 1.3 核函数
核函数（Kernel Function）即 SIMT 架构中多个线程执行的“同一份代码”，这段代码在 Device 上运行，用NVCC编译，产生的机器码是GPU的机器码。<b>所以写CUDA程序就是写核函数：1.确保 kernel function 能产生正确的结果；2.根据硬件特性和任务优化 kernel function 部分。</b>
> SIMD vs SIMT
> SIMD 单指令多数据属于向量机制，比如，四个数分别加上另外四数字，那么 SIMD 将四个数字到一个向量指令，一条指令即可做四次 add 指令的工作。但是，这种机制过于死板，不允许每个分支有不同的操作，每次必须填充满（比较三个数，但向量指令长度为四，需要填充一个数）。
> 
> 相比之下，SIMT 单指令多线程就更加灵活了，虽然两者都是将相同指令广播给多个执行单元，但是 SIMT 的某些线程可以选择不执行，也就是说，同一时刻所有线程被分配给相同的指令，<b>SIMD 必须执行，而 SIMT 可以根据需要不执行</b>。这样 SIMT 保证了线程级并行，而 SIMD 则是指令级并行。
> 
> 总结如下，SIMT 包括以下 SIMD 不具有的关键特性：
> 
> 1. 每个 Thread 有单独的指令地址计数器
> 2. 每个 Thread 有单独的寄存器状态
> 3. 每个 Thread 可以有一个独立的执行路径
> 
> 而上面这3个特性在编程模型可用的方式就是：给每个线程一个唯一的标号（blckIdx,threadIdx），并且这三个特性保证了各线程之间的独立。

#### 1.3.1 核函数调用
核函数的调用通过以下 ANSI C 扩展出的CUDA C指令：
``` c
// 
kernel_name<<<grid,block>>>(argument list);
```

下面是一个示例，当我们配置 <<<4, 8>>>，即 block = 4， thread = 8 时，kernel 的线程分配如下所示：
![Desktop View](/assets/img/blog/CUDA/thread-2.png){: width="800" height="450" }
<center><i>Fig 4.</i> CUDA 线程布局示例</center>

<b>核函数是同时复制到多个线程执行。</b>为了让多线程按照我们的意愿对应到不同的数据，就要给线程一个唯一的标识，由于设备内存是线性的，我们可以根据 `threadIdx` 和 `blockIdx` 来组合获得对应的线程的唯一标识，如 *Fig 5* 所示。

![Desktop View](/assets/img/blog/CUDA/thread-3.png){: width="550" height="450" }
<center><i>Fig 5.</i> CUDA 线程布局示例2</center>

当主机 (Host) 启动核函数之后，控制权马上回到主机，而不是主机等待设备完成核函数的运行。如果需要主机 (Host) 等待设备端执行，可以使用下面这个指令。
``` c++
cudaError_t cudaDeviceSynchronize(void);
```

#### 1.3.2 核函数编写
核函数声明模板：`__global__ void kernel_name(argument list);`。CUDA C中还有一些其他的限定符，如下表所示。

|    限定符    |     执行      |                                            调用                                             | 备注                      |
| :----------: | :-----------: | :-----------------------------------------------------------------------------------------: | ------------------------- |
| \_\_global__ | Device 端执行 | 可以从主机调用<br />也可以从 [计算能力](https://developer.nvidia.cn/cuda-gpus) ≥3的设备调用 | 必须有一个void的返回类型  |
| _\_device__  | Device 端执行 |                                         设备端调用                                          | -                         |
|  _\_host__   |  Host 端执行  |                                          主机调用                                           | 可以省略 \_\_host\_\_标识 |

kernel 函数编写有以下限制：（1）只能访问 Device 端内存；（2）必须有void返回类型；（3）不支持可变数量的参数；（4）不支持静态变量；（5）显示异步行为。

关于 Kernel 函数正确性验证，通常使用 CPU 代码比较计算精度。

#### 1.3.3 核函数计时
通常的 C 语言程序计时如下，一般调用系统 lib 中的cpu计时器函数，例如 Linux 中的 `gettimeofday(&tp,NULL)`。

``` c
#include <time.h>

clock_t s, e;
s = clock();
// ... 运行程序
e = clock();
double duration = (double)(finish - start) / CLOCKS_PER_SEC;
```

但是，由于核函数与主机程序是异步的（核函数一旦启动，控制权交还至主机），这样统计的时间会偏大。即使在 `e = clock()` 前添加 `cudaDeviceSynchronize();` 等待同步，结果仍然会偏大。添加同步后仍然偏大的原因如下图 *Fig 6* 所示。

![Desktop View](/assets/img/blog/CUDA/time.png){: width="550" height="650" }
<center><i>Fig 5.</i> CUDA 线程布局示例2</center>

一般而言，使用 CUDA C 内置的 API 为核函数计时，一般在计时前会 warm up 运行几次，然后多次运行求平均时间。
``` c++
  /* warm up */
  cudaEvent_t start, stop;
  cudaEventCreate(&start);
  cudaEventCreate(&stop);

  cudaEventRecord(start);
  /* kernel func */
  cudaEventRecord(stop);
  cudaEventSynchronize(stop);

  float milliseconds = 0;
  cudaEventElapsedTime(&milliseconds, start, stop);
```

### 1.4 错误处理
错误处理目的是帮助定位 bug 位置。CUDA 常用错误处理函数如下所示。

``` c++
#define CHECK_CUDA(call)\
{\
  const cudaError_t error=call;\
  if(error!=cudaSuccess)\
  {\
      printf("ERROR: %s:%d,",__FILE__,__LINE__);\
      printf("code:%d,reason:%s\n",error,cudaGetErrorString(error));\
      exit(1);\
  }\
}
```

## 2. CUDA 执行模型
> CUDA 目的是充分压榨硬件性能，提高计算效率，需要掌握不同硬件特性，进行针对性优化。

### 2.1 流式处理器（SM）
GPU 架构是围绕流式多处理器（SM）的扩展阵列搭建的，通过复制这种结构来实现硬件并行。GPU 中每个 SM 都能支持数百个线程并发执行，每个 GPU 通常有多个 SM。

如 *Fig 6* 所示，当一个 kernel function 被启动时，<b>多个 Block 会被同时分配到可用的 SM 上执行</b>。Block 被分配到某一个 SM 上以后，将分为多个线程束，每个线程束一般是 32 个线程（目前的 GPU 都是32个线程，但不保证未来还是 32 个）。当一个 Blcok 被分配给一个 SM 后，它就只能在这个 SM 上执行了，不会重新分配到其他 SM 上了，多个线程块可以被分配到同一个 SM 上。

![Desktop View](/assets/img/blog/CUDA/sm.png){: width="675" height="450" }
<center><i>Fig 6.</i> 线程映射关系</center>

Block 是逻辑产物，因为在计算机里，内存总是一维线性存在的，所以执行起来也是一维的访问线程块中的线程。编程模式中，二维三维的线程块，只是方便写程序。
图 *Fig 7* 从逻辑和硬件角度描述了 CUDA 编程模型对应的硬件。

![Desktop View](/assets/img/blog/CUDA/sm-2.png){: width="500" height="450" }
<center><i>Fig 7.</i> CUDA 编程模型与硬件对应关系</center>

SM 中的**共享内存**和**寄存器**是关键的资源，一个 Block 中的 Threads 通过共享内存和寄存器相互通信协调。**寄存器和共享内存的分配可以严重影响性能！**

因为 SM 有限，虽然从 CUDA 编程模型层面看，所有 Thread 都是并行执行的，但是在微观上看，所有 Block 是分批次的在物理层面的机器上执行，Block 里不同的Thread 可能进度都不一样（因为可能被分到不同的 Warp 上）。但是，<b>同一个 Warp 内的线程拥有相同的进度。</b>

并行就会引起竞争，多线程以未定义的顺序访问同一个数据，就导致了不可预测的行为，CUDA 只提供了 Block 内同步机制，Block 之间无法同步。

### 2.2 线程束（Warp）
基本所有设备线程束维持在 32 （切割线程束按照 x 方向切，即一个线程束中的threadIdx.x 连续变化），也就是说，每个SM上有多个 Block，一个 Block 有多个线程（可以是几百个，但不会超过某个最大值）；但是，从机器的角度，<b>在某时刻 T，SM上只执行一个线程束，也就是32个线程在同时同步执行，线程束中的每个线程执行同一条指令，包括有分支的部分</b>。

#### 2.2.1 线程束分化
由于 SIMT 的特性，**Warp 分化会产生严重的性能下降。** Warp 分化即同一个 Warp 中的线程，执行不同的指令，例如下面代码示例所示，当一个 Warp 执行时，如果 16个 Thread 执行 if 中的代码，而另外 16 个执行 else 中的代码，则产生 Warp 分化。**条件分支越多，并行性削弱越严重。**

解决方案：从 Warp 角度去解决，根本思路是避免同一个线程束内的线程分化。补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；<b>只有当线程束内有分歧产生分支的时候，性能才会急剧下降。</b>

例如，假设 Block = 64，下面是一个比较低效的分支
``` c++
__global__ void mathKernel1(float *c)
{
	int tid = blockIdx.x* blockDim.x + threadIdx.x;

	float a = 0.0;
	float b = 0.0;
    // 编译器会优化，可以使用 bool ipred = (tid % 2 == 0); 替代条件判断
	if (tid % 2 == 0)
	{
		a = 100.0f;
	}
	else
	{
		b = 200.0f;
	}
	c[tid] = a + b;
}
```
换一种方法，得到相同但是错乱的结果（顺序可以后期调整），那么下面代码就会很高效。
* 第一个线程束内的线程编号 tid 从 0 到 31，tid / warpSize 都等于0，那么就都执行 if 语句；
* 第二个线程束内的线程编号 tid 从 32 到 63，tid / warpSize 都等于1，执行 else；

```
__global__ void mathKernel2(float *c)
{
	int tid = blockIdx.x* blockDim.x + threadIdx.x;
	float a = 0.0;
	float b = 0.0;
	if ((tid/warpSize) % 2 == 0)
	{
		a = 100.0f;
	}
	else
	{
		b = 200.0f;
	}
	c[tid] = a + b;
}
```

#### 2.2.2 资源分配
如上文所述，每个 SM 上执行的基本单位是 Warp，指令调度器会将同一条指令广播给 Warp 内的全部线程。当出现分支发散时，Warp 会通过线程掩码控制部分线程执行、部分线程闲置（逻辑上不执行当前指令，但 Warp 整体依然在执行），直至分支路径全部完成。从 SM 调度角度看，Warp 可以分为：（1）已激活。已分配到 SM，具备执行所需资源，但可能因等待数据或资源而暂时阻塞；（2）未激活。Block 尚未被分配到 SM，Warp 尚未加载到片上执行。

Warp 一旦被激活来到片上，就不会再离开 SM 直到执行结束。而每个 SM 上有多少个线程束处于激活状态，取决于以下资源：
- 程序计数器
- 寄存器
- 共享内存

换句话说，一个 SM 上能激活多少个 Block 和 Warp（Warp是更细的激活粒度，Block -> 多个 Warp 分组）取决于 SM 中可用的寄存器和共享内存，以及 Kernel 所需的寄存器和共享内存大小。这是一个 tradeoff，当kernel 占用的资源较少，那么更多的线程（这是线程越多 Warp 也就越多）处于活跃状态，相反则线程越少，如图 *Fig 8* 所示。

![Desktop View](/assets/img/blog/CUDA/resource-reg.png){: width="675" height="450" }
![Desktop View](/assets/img/blog/CUDA/resource-shared_mem.png){: width="675" height="450" }
<center><i>Fig 8.</i> 资源分配 Tradeoff 示例 </center>

当寄存器和共享内存分配给了 Block，这个 Block 处于活跃状态，所包含的 Warp 称为活跃线程束。活跃的线程束可分为三类：
- 选定的线程束
- 阻塞的线程束
- 符合条件的线程束

​当 SM 执行某个线程束时，执行的这个线程束叫做选定的线程束，准备要执行的叫符合条件的线程束，如果线程束不符合条件还没准备好则为阻塞的线程束。
​满足下面的要求，线程束才算是符合条件的：（1）32个CUDA核心可以用于执行；（2）执行所需要的资源全部就位。

​由于计算资源是在 Warp 之间分配的，且 Warp 的整个生命周期都在片上，所以 Warp 上下文切换是非常快速的。因此，如可以通过排布流水，通过大量活跃的线程束切换来实现延迟隐藏。

所以，**延迟的隐藏取决于活动的线程束的数量，数量越多**，隐藏的越好，但是线程束的数量又受到上面的说的资源影响。所以这里就需要寻找最优的执行配置来达到最优的延迟隐藏。

### 2.3 避免分支分化
下面将用一个 add 归约的示例进行说如如何解决分支分化的问题。归约的方法基本包括以下几个步骤：（1）将输入向量划分到更小的数据块中；（2）用一个线程计算一个数据块的部分和；（3）对每个数据块的部分和再求和得到最终的结果。

流水线思想。一般考虑两类延迟：算术延迟、内存延迟。下图就是阻塞线程束到可选线程束的过程逻辑图：




## Reference
[1] https://face2ai.com/program-blog/
